{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Data Warehouse\n",
    "Samuel Botter Martins\n",
    "\n",
    "## Set up AWS resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook aims to setup _all AWS resources_ for the project. This includes creating:\n",
    "- an IAM role and assigned to my IAM user\n",
    "- creating a Redshift cluster with a default database\n",
    "- testing the connection accessing the database\n",
    "\n",
    "I followed almost the same steps as in Exercise 2 on _Infrastructure as Code_ but with some adaptations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import configparser\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start\n",
    "First, I followed the same steps as in the Exercise of IaC and created a new IAM user in my AWS account with _AdministrationAccess_. <br/>\n",
    "I then used my _security credentials (access keys)_ to follow this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load DWH Parameters from a configuration file\n",
    "My AWS configuration is placed in the `dwh.cfg` file. For security reasons, I have just provided a template of this file called `dwh_template.cfg`.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "# AWS\n",
    "KEY = config.get('AWS', 'KEY')\n",
    "SECRET = config.get('AWS', 'SECRET')\n",
    "REGION = config.get('AWS', 'REGION')\n",
    "\n",
    "# IAM\n",
    "IAM_ROLE_NAME = config.get('IAM', 'ROLE_NAME')\n",
    "\n",
    "# DWS\n",
    "DWH_CLUSTER_TYPE = config.get('DWH', 'DWH_CLUSTER_TYPE')\n",
    "DWH_NUM_NODES = config.get('DWH', 'DWH_NUM_NODES')\n",
    "DWH_NODE_TYPE = config.get('DWH', 'DWH_NODE_TYPE')\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get('DWH', 'DWH_CLUSTER_IDENTIFIER')\n",
    "DWH_DB = config.get('DWH', 'DWH_DB')\n",
    "DWH_DB_USER = config.get('DWH', 'DWH_DB_USER')\n",
    "DWH_DB_PASSWORD = config.get('DWH', 'DWH_DB_PASSWORD')\n",
    "DWH_PORT = config.get('DWH', 'DWH_PORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the loaded parameters\n",
    "# uncomment the code below to show your credentials\n",
    "\n",
    "# pd.DataFrame({\n",
    "#     \"Param\": ['KEY', 'SECRET', 'REGION', 'IAM_ROLE_NAME', 'DWH_CLUSTER_TYPE', 'DWH_NUM_NODES', 'DWH_NODE_TYPE', 'DWH_CLUSTER_IDENTIFIER', 'DWH_DB', 'DWH_DB_USER', 'DWH_DB_PASSWORD', 'DWH_PORT'],\n",
    "#     \"Value\": [KEY, SECRET, REGION, IAM_ROLE_NAME, DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT]\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. IAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating a client for IAM\n",
    "Creating a python client to access AWS IAM from the user specified in the `dwg.cfg` configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client('iam',\n",
    "                   region_name=REGION,\n",
    "                   aws_access_key_id=KEY,\n",
    "                   aws_secret_access_key=SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 IAM Role\n",
    "#### 2.2.1 *Create an IAM Role*\n",
    "This makes Redshift able to access the S3 bucket (_ReadOnly_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Creating a new IAM Role\") \n",
    "    \n",
    "    dwh_role = iam.create_role(\n",
    "                    Path='/',\n",
    "                    # name given in my configuration file\n",
    "                    RoleName=IAM_ROLE_NAME,\n",
    "                    Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "                    AssumeRolePolicyDocument = json.dumps({\n",
    "                        'Statement': [{\n",
    "                            'Action': 'sts:AssumeRole',\n",
    "                            'Effect': 'Allow',\n",
    "                            'Principal': {'Service': 'redshift.amazonaws.com'}\n",
    "                        }],\n",
    "                        'Version': '2012-10-17'\n",
    "                    })\n",
    "                )    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check this created _role_:\n",
    "- Go to _IAM_ dashboard on AWS.\n",
    "- In the navigation panel, choose **Rules**\n",
    "- You will see the _created role_ in the list of Roles\n",
    "\n",
    "Click on this _role_ to see its properties. <br/>\n",
    "Note that **there is no policy assigned to this role yet**. That's exactly what we're going to do now.\n",
    "\n",
    "To _**delete**_ this role, select it in the list and delete it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 *Assigning _policies_ to an IAM Role*\n",
    "We will assign a policy to our role that allows Redshift to:\n",
    "    - `AmazonS3ReadOnlyAccess`: access S3 buckets (ReadOnly).\n",
    "    - `AmazonRedshiftQueryEditor`: access Redshift Query Editor.\n",
    "      - Required to use the _Query Editor_ on AWS and the _SQL Workbench_ tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attaching Policy: AmazonS3ReadOnlyAccess\")\n",
    "iam.attach_role_policy(\n",
    "            RoleName=IAM_ROLE_NAME,\n",
    "            PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    ")['ResponseMetadata']['HTTPStatusCode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return `200` means that worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attaching Policy: AmazonS3ReadOnlyAccess\")\n",
    "iam.attach_role_policy(\n",
    "            RoleName=IAM_ROLE_NAME,\n",
    "            PolicyArn=\"arn:aws:iam::aws:policy/AmazonRedshiftQueryEditor\"\n",
    ")['ResponseMetadata']['HTTPStatusCode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go again in the properties of the role in AWS to verify that the policies created were correctly assigned to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 *Get the IAM role ARN (Amazon Resource Names)\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_arn = iam.get_role(RoleName=IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **ARN** _uniquely identifies_ AWS resources. In this case, we have the ARN for our function!\n",
    "\n",
    "We will use this **ARN** when creating the Redshift cluster.\n",
    "\n",
    "To use this function in future code, we saved it as a setting in our `dwh.cfg` configuration file: `[IAM] ROLE_ARN`.\n",
    "\n",
    "<br/> <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Redshift Cluster\n",
    "### 3.1 Creating a client for Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift = boto3.client('redshift',\n",
    "                        region_name=REGION,\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating a **Redshift Cluster**\n",
    "See the [official docs](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift/client/create_cluster.html#).\n",
    "\n",
    "We need to create the **first database** when the cluster is created using the `DBName` parameter. To create additional databases after the cluster is created, connect to the cluster with an SQL client and use SQL commands to create a database. See the [docs](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift/client/create_cluster.html#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster( \n",
    "                    # Data Warehouse specs\n",
    "                    ClusterType=DWH_CLUSTER_TYPE,\n",
    "                    NodeType=DWH_NODE_TYPE,\n",
    "                    NumberOfNodes=int(DWH_NUM_NODES),\n",
    "        \n",
    "                    # first database of the cluster\n",
    "                    DBName=DWH_DB,\n",
    "                    ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "                    MasterUsername=DWH_DB_USER,\n",
    "                    MasterUserPassword=DWH_DB_PASSWORD,\n",
    "\n",
    "                    # my user's role to manage this Redshift (for S3)\n",
    "                    IamRoles=[role_arn]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the created Redshift cluster:\n",
    "- Access the _Amazon Redshift_ dashboard on AWS.\n",
    "- Select the considered _region_ from the menu bar (in my case _us-east-1_)\n",
    "- In the navigation pane, choose **Clusters**\n",
    "- You will see the _cluster created_ in the _Clusters_ list\n",
    "- It may take some time to finish creating the cluster (_Available_ status)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the **cluster _status_** on the cluster dashboard in AWS Redshift. When the status becomes **Available**, follow the instructions below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Click on this _cluster_ to see its **properties**. <br/>\n",
    "We can see the _cluster specifications_ and _other information_ on **General information**, such as:\n",
    "- cluster identification\n",
    "- Cluster ARN\n",
    "- Node type\n",
    "- Number of nodes\n",
    "- Endpoint, etc\n",
    "\n",
    "We can manually copy any of this information to use in our future code, but we'll do it _directly_ from the code.\n",
    "\n",
    "In the **Properties tab** we have other useful information:\n",
    "- Database configurations\n",
    "- Network and security settings\n",
    "- Associated IAM roles \n",
    "  - **Check our role there**\n",
    "- Node IP addresses\n",
    "  - This is interesting because we can see _all the nodes_ needed (in our case 4) and the _leader_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the cluster properties by code\n",
    "Instead of opening the AWS panel to get the cluster properties, we can do this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe cluster properties with identifier DWH_CLUSTER_IDENTIFIER\n",
    "# from our redshift python client\n",
    "cluster_props = redshift.describe_clusters(\n",
    "    ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print only a few properties as a pandas dataframe\n",
    "def pretty_redshift_props(props):\n",
    "    prop_keys_to_show = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    \n",
    "    x = [(k, v) for k,v in props.items() if k in prop_keys_to_show]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "pretty_redshift_props(cluster_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "#### Deleting a cluster\n",
    "To _**delete**_ this cluster, go to the AWS Redshift cluster dashboard, select the cluster in the list and delete it! Otherwise, do that in code (see the jupyter notebook `teardown_AWS_resources.ipynb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.3 Get the cluster <font color='red'> endpoint (host) and role ARN </font> </h3>\n",
    "<font color='red'>DO NOT RUN THIS unless the cluster status becomes \"Available\".</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DWH_ENDPOINT = cluster_props['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = cluster_props['IamRoles'][0]['IamRoleArn']\n",
    "\n",
    "print(f\"DWH_ENDPOINT: {DWH_ENDPOINT}\")\n",
    "print(f\"DWH_ROLE_ARN: {DWH_ROLE_ARN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this information can be retrieved from the cluster properties on the AWS platform, as we did earlier. The `DWH_ROLE_ARN` is exactly the same `ROLE_ARNA` we got before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our ETL codes easier, I will save the `DWH_ENDPOINT` in the `dwh.cfg` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Open an incoming **TCP port** _to access_ the cluster endpoint\n",
    "To access the cluster through its endpoint, we need to open an inbound TCP port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a EC2 client\n",
    "ec2 = boto3.resource('ec2',\n",
    "                     region_name=REGION,\n",
    "                     aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=cluster_props['VpcId'])\n",
    "\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        # information insider the file `dwh.cfg`\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Checking cluster connection\n",
    "The course used `pyscopg2` to connect to databases in Redshift. However, **it is outdated**.\n",
    "\n",
    "Instead, we will use the `redshift_connector` package according to the [official AWS tutorial](https://docs.aws.amazon.com/redshift/latest/mgmt/python-connect-examples.html).\n",
    "   it is now out of date to connect to redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install redshift_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_connector\n",
    "\n",
    "conn = redshift_connector.connect(\n",
    "     host=DWH_ENDPOINT,\n",
    "     database=DWH_DB,\n",
    "     port=DWH_PORT,\n",
    "     user=DWH_DB_USER,\n",
    "     password=DWH_DB_PASSWORD\n",
    "  )\n",
    "\n",
    "conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Connecting by SQL Workbench**\n",
    "Install and configure _SQL Workbench_ to access your _Redshift Cluster_. I provided a simple tutorial for that in the [SQL_workbench_on_Redshift.md](./SQL_workbench_on_Redshift.md) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. READY FOR ETL\n",
    "If everything worked so far, then now we can run our ETL process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleaning the resources by code\n",
    "Keeping all active costs of created resources! So after finishing the project, running ETC, etc, we need to clean up/delete _all_ resources allocated in AWS. <br/>\n",
    "One option is to do this _manually_ on the AWS platform. However, we can do this by code!\n",
    "\n",
    "I created the jupyter-notebook `tear_down_AWS_resources.ipynb` to clean up/dismount all AWS resources directly from code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
